{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78979de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e80b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "Model = namedtuple('Model', 'type, name, dimension, corpus, model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c0632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/complex-word-identification-dataset/cwishareddataset.zip\"\n",
    "filename = \"cwishareddataset.zip\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc3c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = \"cwishareddataset.zip\"  # Ensure this is the correct filename\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")  # Extract to the current directory\n",
    "\n",
    "print(\"Extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6989f50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: traindevset/english/Wikipedia_Train.tsv -> Exists? True\n",
      "Checking: traindevset/english/Wikipedia_Dev.tsv -> Exists? True\n",
      "Checking: traindevset/english/WikiNews_Train.tsv -> Exists? True\n",
      "Checking: traindevset/english/WikiNews_Dev.tsv -> Exists? True\n",
      "Checking: traindevset/english/News_Train.tsv -> Exists? True\n",
      "Checking: traindevset/english/News_Dev.tsv -> Exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MAIN_PATH_DATASET = \"traindevset/english/\"\n",
    "\n",
    "# Check for all expected files\n",
    "expected_files = ['Wikipedia_Train.tsv', 'Wikipedia_Dev.tsv',\n",
    "                  'WikiNews_Train.tsv', 'WikiNews_Dev.tsv',\n",
    "                  'News_Train.tsv', 'News_Dev.tsv']\n",
    "\n",
    "for file in expected_files:\n",
    "    file_path = os.path.join(MAIN_PATH_DATASET, file)\n",
    "    print(f\"Checking: {file_path} -> Exists? {os.path.exists(file_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794dcfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "MAIN_PATH_DATASET = \"traindevset/english/\"\n",
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]\n",
    "\n",
    "feature_categories = []\n",
    "\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ef414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "filename = \"glove.6B.zip\"\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Server is down. Try again later.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1def28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"glove.6B.zip\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(\"Corrupted file deleted. Download it again.\")\n",
    "else:\n",
    "    print(\"No corrupted file found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"glove.6B.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"glove/\")\n",
    "\n",
    "print(\"Extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "print(\"Files in Directory:\", os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7080b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "source = \"C:/Users\\pc\\Downloads\\glove.6B.zip\"  # Update this with the actual location\n",
    "destination = os.getcwd()  # Moves to your current working directory\n",
    "\n",
    "shutil.move(source, destination)\n",
    "print(\"File moved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b88636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "MAIN_PATH = 'embeddings/'\n",
    "\n",
    "\n",
    "glove_models = []\n",
    "\n",
    "glove_defs = [ Model('glove', 'glove.6B.300d.txt', 300, 'wikipedia+gigaword5', None)]\n",
    "              \n",
    "for model in glove_defs:\n",
    "    glove_file = MAIN_PATH + model.name\n",
    "    tmp_file = get_tmpfile(model.name + '-temp')\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    glove_models.append(Model(model.type, model.name, model.dimension, model.corpus, vecs))\n",
    "    print('load model : {}'.format(model.name))\n",
    "    \n",
    "print(glove_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"embeddings/glove.6B.300d.txt\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File found:\", file_path)\n",
    "else:\n",
    "    print(\"File not found! Check your directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58477d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_glove_file(root_folder):\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        if \"glove.6B.300d.txt\" in filenames:\n",
    "            return os.path.join(dirpath, \"glove.6B.300d.txt\")\n",
    "    return None\n",
    "\n",
    "file_path = find_glove_file(\"C:/\")  # Change to \"/\" for Mac/Linux\n",
    "\n",
    "if file_path:\n",
    "    print(\"Found GloVe file at:\", file_path)\n",
    "else:\n",
    "    print(\"File not found. Please download it again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7255b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = \"glove.6B.zip\"\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"embeddings/\")\n",
    "    print(\"Extraction complete!\")\n",
    "else:\n",
    "    print(\"ZIP file not found. Download it first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"embeddings/glove.6B.300d.txt\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"✅ File is ready to use!\")\n",
    "else:\n",
    "    print(\"❌ File is still missing. Check again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e53c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = \"embeddings/glove.6B.300d.txt\"\n",
    "tmp_file = \"embeddings/glove.6B.300d.word2vec\"\n",
    "\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31749edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = datasets[0].train[0:30]\n",
    "\n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "def extract_ngrams_group(group):\n",
    "    targets = zip(group['target'].values.tolist(), group['start'].values.tolist(),\n",
    "                 group['end'].values.tolist(), group['binary'].values.tolist())\n",
    "    for word, start, end, binary in targets:\n",
    "        tokens = word.split()\n",
    "        if len(tokens)>1:\n",
    "            olap_words = [(w, b) for w, s, e, b in targets if overlaps(start, end, s, e)]\n",
    "            \n",
    "    \n",
    "grouped = dataframe.groupby('sentence').apply(lambda group : extract_ngrams_group(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95abf51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_lowercased = set(i.lower() for i in brown.words())\n",
    "print (len(wordlist_lowercased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def all_tokens_with_index(context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    return [val for val in targets if val[0] != '\"']\n",
    "\n",
    "def build_vocabulary(sentences, embedding_model, dimension):\n",
    "    all_words = [tpl[0] for sentence in sentences for tpl in sentence['seq']] + list(wordlist_lowercased)\n",
    "    print('# Words : {}'.format(len(all_words)))\n",
    "    counter = Counter(all_words)\n",
    "    vocab_size = len(counter) + 1\n",
    "    print('# Vocab : {}'.format(vocab_size))\n",
    "    print('# embeding model  : {}'.format(len(embedding_model.vocab)))   \n",
    "    word2index = {word : index for index, (word, count) in enumerate(counter.most_common(), 1)}\n",
    "    index2word = {index : word for word, index in word2index.items()}\n",
    "    # +1 required for pad token\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    missing_embed_words = []\n",
    "    i_ = 0\n",
    "    for word, index in word2index.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            embedding = embedding_model[word]\n",
    "        else:\n",
    "             i_ +=1\n",
    "             continue\n",
    "        embedding_matrix[index] = embedding\n",
    "    missing_embed_count = len(missing_embed_words)\n",
    "    print('# Words missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    print(\"i: \", i_ )\n",
    "    return word2index, index2word, embedding_matrix\n",
    "\n",
    "def forward_transformation(dataframe, lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    grouped = dataframe.groupby('sentence').apply(lambda row : \n",
    "                        {'sent_id' : list(set(row['sent_id']))[0],\n",
    "                         'sentence' : list(set(row['sentence']))[0], \n",
    "                         'tags': [tag for tag in zip(row['target'], \n",
    "                            row['start'], row['end'], row['binary'], row['prob'])]})\n",
    "    sentences = []\n",
    "    for vals in grouped:\n",
    "        sent_id = vals['sent_id']\n",
    "        sentence = vals['sentence']\n",
    "        tags = vals['tags']\n",
    "        tags_without_labels = [(word, start, end) for word, start, end, binary, prob in tags]\n",
    "        all_tokens = all_tokens_with_index(sentence)\n",
    "        sent_repr = [(word, start, end, tags[tags_without_labels.index((word, start, end))][3],\n",
    "                     tags[tags_without_labels.index((word, start, end))][4])\n",
    "           if (word, start, end) in tags_without_labels \n",
    "          else (word, start, end, 0, 0.0) for word, index, start, end in all_tokens]\n",
    "        if lowercase:\n",
    "            sent_repr = [(word.lower(), start, end, binary, prob) \n",
    "                         for word, start, end, binary, prob in sent_repr]\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentences.append({'sent_id' : sent_id, 'sentence' : sentence, 'seq' : sent_repr})\n",
    "    return sentences\n",
    "\n",
    "def split_sentence_seqs(sentences):\n",
    "    words, start_end, binary, prob = [], [], [] ,[]\n",
    "    for sent in sentences:\n",
    "        sequence = sent['seq']\n",
    "        curr_w, curr_se, curr_b, curr_p = map(list, zip(*[(vals[0], \n",
    "            (vals[1], vals[2]), vals[3], vals[4]) for vals in sequence]))\n",
    "        words.append(curr_w)\n",
    "        start_end.append(curr_se)\n",
    "        binary.append(curr_b)\n",
    "        prob.append(curr_p)\n",
    "    return words, start_end, binary, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe55c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.append(Dataset('train_all_test_wiki', \n",
    "        datasets[0].train.append(datasets[1].train).append(datasets[2].train), datasets[0].test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append train and test set\n",
    "dataset_sel = datasets[3]\n",
    "train_num_rows = dataset_sel.train.shape[0]\n",
    "train_num_sents = len(list(set(dataset_sel.train.sentence.values.tolist())))\n",
    "\n",
    "test_num_rows = dataset_sel.test.shape[0]\n",
    "test_num_sents = len(list(set(dataset_sel.test.sentence.values.tolist())))\n",
    "\n",
    "dataset = dataset_sel.train.append(dataset_sel.test)\n",
    "dataset['sent_id'] = dataset.groupby('sentence').ngroup()\n",
    "dataset_num_rows = dataset.shape[0]\n",
    "dataset_num_sents = len(list(set(dataset.sentence.values.tolist())))\n",
    "\n",
    "print('# Rows train : {}'.format(train_num_rows))\n",
    "print('# Rows test : {}'.format(test_num_rows))\n",
    "print('# Rows dataset : {}'.format(dataset_num_rows))\n",
    "\n",
    "print('# Sents train : {}'.format(train_num_sents))\n",
    "print('# Sents test : {}'.format(test_num_sents))\n",
    "print('# Sents dataset : {}'.format(dataset_num_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc86e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = forward_transformation(dataset)\n",
    "train_sentences = sentences[:train_num_sents]\n",
    "test_sentences = sentences[train_num_sents:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, start_end, binary, prob = split_sentence_seqs(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lens = [len(sent) for sent in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(sentences, embedding_model, dimension):\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "\n",
    "    counter = Counter()\n",
    "    total_words = 0\n",
    "\n",
    "    for item in sentences:\n",
    "        if isinstance(item, dict):\n",
    "            if 'sentence' in item:\n",
    "                words = item['sentence'].lower().split()  # Tokenize sentence\n",
    "            elif 'seq' in item:\n",
    "                words = [word_info[0] for word_info in item['seq']]  # Extract words from 'seq'\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected dictionary format: {item}\")\n",
    "\n",
    "            counter.update(words)\n",
    "            total_words += len(words)  # Track total words\n",
    "        else:\n",
    "            raise TypeError(f\"Expected dictionary, but got {type(item)}: {item}\")\n",
    "\n",
    "    vocab_size = len(counter) + 1  # +1 for padding/indexing consistency\n",
    "    print(\"# Words : {}\".format(total_words))\n",
    "    print(\"# Vocab : {}\".format(vocab_size))\n",
    "    print(\"# embeding model  : {}\".format(len(embedding_model.key_to_index)))\n",
    "\n",
    "    word2index = {word: index for index, (word, count) in enumerate(counter.most_common(), 1)}\n",
    "    index2word = {index: word for word, index in word2index.items()}\n",
    "\n",
    "    embedding = np.zeros((vocab_size, dimension))\n",
    "    missing_words = 0\n",
    "\n",
    "    for word, index in word2index.items():\n",
    "        if word in embedding_model.key_to_index:\n",
    "            embedding[index] = embedding_model[word]\n",
    "        else:\n",
    "            missing_words += 1\n",
    "\n",
    "    print(\"# Words missing embedding : {}\".format(missing_words))\n",
    "    print(\"Embedding shape : {}\".format(embedding.shape))\n",
    "    print(\"i: \", word2index.get('i', 'Not found'))  # Print the index of \"i\" if it exists\n",
    "\n",
    "    return word2index, index2word, embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13558bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, index2word, embedding = build_vocabulary(sentences, embedding_model, dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_words = 0\n",
    "\n",
    "for word, index in word2index.items():\n",
    "    cleaned_word = word.lower().strip()  # Normalize word\n",
    "    if cleaned_word in embedding_model.key_to_index:\n",
    "        embedding[index] = embedding_model[cleaned_word]\n",
    "    else:\n",
    "        missing_words += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = word.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aed82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word = re.sub(r'[^a-zA-Z]', '', word)  # Keep only alphabets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_words = [word for word in word2index.keys() if word not in embedding_model.key_to_index]\n",
    "print(\"Sample missing words:\", missing_words[:20])\n",
    "print(\"Total missing words:\", len(missing_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_words = 0\n",
    "for word, index in word2index.items():\n",
    "    cleaned_word = word.lower().strip()  # Ensure consistency\n",
    "    if cleaned_word in embedding_model.key_to_index:\n",
    "        embedding[index] = embedding_model[cleaned_word]\n",
    "    else:\n",
    "        missing_words += 1\n",
    "\n",
    "print(\"Final missing words:\", missing_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc167c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_word(word):\n",
    "    return re.sub(r'[^a-zA-Z]', '', word).lower()  # Remove numbers/punctuation\n",
    "\n",
    "missing_words = 0\n",
    "for word, index in word2index.items():\n",
    "    cleaned_word = clean_word(word)\n",
    "    if cleaned_word in embedding_model.key_to_index:\n",
    "        embedding[index] = embedding_model[cleaned_word]\n",
    "    else:\n",
    "        missing_words += 1\n",
    "\n",
    "print(\"Missing words after cleaning:\", missing_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "\n",
    "# Function to normalize words\n",
    "def normalize_word(word):\n",
    "    word = str(word).lower()  # Convert to lowercase\n",
    "    word = re.sub(r\"[^a-zA-Z0-9]\", \"\", word)  # Remove special characters\n",
    "    return word\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocabulary(sentences, embedding_model, dimension):\n",
    "    counter = Counter()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if isinstance(sentence, dict) and 'seq' in sentence:  # Ensure sentence is a valid dict with 'seq' key\n",
    "            words = [normalize_word(word[0]) for word in sentence['seq']]  # Extract and normalize words\n",
    "            counter.update(words)\n",
    "        else:\n",
    "            raise TypeError(f\"Expected dict with 'seq' key, but got {type(sentence)}: {sentence}\")\n",
    "\n",
    "    vocab_size = len(counter) + 1\n",
    "    print('# Vocab : {}'.format(vocab_size))\n",
    "    print('# embedding model  : {}'.format(len(embedding_model.key_to_index)))  \n",
    "\n",
    "    # Create word-to-index mappings\n",
    "    word2index = {word: index for index, (word, count) in enumerate(counter.most_common(), 1)}\n",
    "    index2word = {index: word for word, index in word2index.items()}\n",
    "\n",
    "    # Initialize embedding matrix\n",
    "    embedding = np.zeros((vocab_size, dimension))\n",
    "    missing_words = 0\n",
    "\n",
    "    for word, index in word2index.items():\n",
    "        if word in embedding_model.key_to_index:\n",
    "            embedding[index] = embedding_model[word]\n",
    "        else:\n",
    "            missing_words += 1  # Track missing words\n",
    "\n",
    "    print(f'# Words missing embedding : {missing_words}')\n",
    "    print(f'Embedding shape : {embedding.shape}')\n",
    "\n",
    "    return word2index, index2word, embedding\n",
    "\n",
    "# Normalize and map words to indices\n",
    "def convert_words_to_indices(words, word2index):\n",
    "    words_with_indices = []\n",
    "    \n",
    "    for sent in words:\n",
    "        indexed_sentence = [word2index.get(normalize_word(word), 0) for word in sent]  # Map to index (default 0 if missing)\n",
    "        words_with_indices.append(indexed_sentence)\n",
    "    \n",
    "    return words_with_indices\n",
    "\n",
    "# Main Execution\n",
    "embedding_model = glove_models[0].model\n",
    "dimension = embedding_model.vector_size\n",
    "\n",
    "word2index, index2word, embedding = build_vocabulary(sentences, embedding_model, dimension)\n",
    "\n",
    "# Normalize and convert words to indices\n",
    "words_with_indices = convert_words_to_indices(words, word2index)\n",
    "\n",
    "# Get sentence lengths\n",
    "sent_lens = [len(sentence['seq']) for sentence in sentences]\n",
    "sent_max_length = np.max(sent_lens)\n",
    "print('Max length sentence : {}'.format(sent_max_length))\n",
    "\n",
    "# Padding sequences\n",
    "words_padded = pad_sequences(maxlen=sent_max_length, sequences=words_with_indices, padding=\"post\", value=0)\n",
    "binary_padded = pad_sequences(maxlen=sent_max_length, sequences=binary, padding=\"post\", value=0)\n",
    "prob_padded = pad_sequences(maxlen=sent_max_length, sequences=prob, padding=\"post\", value=0, dtype=\"float\")\n",
    "\n",
    "# Convert binary labels to categorical format\n",
    "binary_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binary_padded]\n",
    "\n",
    "# Debugging: Print some normalized words and indices\n",
    "print(\"Sample normalized words:\", [[normalize_word(word) for word in sent] for sent in words[:5]])\n",
    "print(\"Sample indexed words:\", words_with_indices[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f66b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (1) Training set\n",
    "train_words_padded = words_padded[:train_num_sents]\n",
    "train_binary_padded = binary_padded[:train_num_sents]\n",
    "train_binary_padded_categorical = binary_padded_categorical[:train_num_sents]\n",
    "train_prob_padded = prob_padded[:train_num_sents]\n",
    "train_start_end = start_end[:train_num_sents]\n",
    "\n",
    "# (2) Test set\n",
    "test_words_padded = words_padded[train_num_sents:]\n",
    "test_binary_padded = binary_padded[train_num_sents:]\n",
    "test_binary_padded_categorical = binary_padded_categorical[train_num_sents:]\n",
    "test_prob_padded = prob_padded[train_num_sents:]\n",
    "test_start_end = start_end[train_num_sents:]\n",
    "\n",
    "print('Training set length : {}'.format(len(train_words_padded)))\n",
    "print('Test set length : {}'.format(len(test_words_padded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabca1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.f1_scores = []\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        targ = self.validation_data[1]\n",
    "        targ = np.array(targ)\n",
    "        shape = targ.shape\n",
    "        targ = targ.reshape((shape[0]*shape[1], shape[2]))\n",
    "        targ = np.argmax(targ, axis = 1)\n",
    "        predict = predict.reshape((shape[0]*shape[1]), shape[2])\n",
    "        predict = np.argmax(predict, axis = 1)\n",
    "        self.f1s=f1_score(targ, predict)\n",
    "        print(\"\\nF1 Score:\")\n",
    "        print(f1_score(targ, np.ones(shape[0]*shape[1])))\n",
    "        self.f1_scores.append(self.f1s)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99469913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "vocab_size = embedding.shape[0]\n",
    "dimension = embedding.shape[1]\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "in_seq = Input(shape=(sent_max_length,))\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "drop = Dropout(0.1)(embed)\n",
    "lstm = Bidirectional(LSTM(units=150, return_sequences=True, recurrent_dropout=0.1))(drop)\n",
    "out = TimeDistributed(Dense(2, activation=\"softmax\"))(lstm) \n",
    "\n",
    "model = Model(in_seq, out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "metrics = Metrics((test_words_padded, np.array(test_binary_padded_categorical)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93188e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_words_padded, np.array(train_binary_padded_categorical), batch_size=10, \n",
    "                    epochs=3, validation_data = (test_words_padded, np.array(test_binary_padded_categorical)), \n",
    "                    verbose=1, callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4775f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(\"Saving to:\", cwd)\n",
    "\n",
    "model_save_name = 'model_CWI_full.h5'\n",
    "path_dir = os.path.join(cwd, model_save_name)\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save(path_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccee34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = 'model_CWI_full.h5'\n",
    "path_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "epoch_f1s = plt.plot(metrics.f1_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ = set(stopwords.words('english'))\n",
    "def cleaner(word):\n",
    "  #Remove links\n",
    "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', \n",
    "                '', word, flags=re.MULTILINE)\n",
    "  word = re.sub('[\\W]', ' ', word)\n",
    "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
    "  return word.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c7831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_input(input_text):\n",
    "  input_text = cleaner(input_text)\n",
    "  clean_text = []\n",
    "  index_list =[]\n",
    "  input_token = []\n",
    "  index_list_zipf = []\n",
    "  for i, word in enumerate(input_text.split()):\n",
    "    if word in word2index:\n",
    "      clean_text.append(word)\n",
    "      input_token.append(word2index[word])\n",
    "    else:\n",
    "      index_list.append(i)\n",
    "  input_padded = pad_sequences(maxlen=sent_max_length, sequences=[input_token], padding=\"post\", value=0)\n",
    "  return input_padded, index_list, len(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_missing_word(pred_binary, index_list, len_list):\n",
    "  list_cwi_predictions = list(pred_binary[0][:len_list])\n",
    "  for i in index_list:\n",
    "    list_cwi_predictions.insert(i, 0)\n",
    "  return list_cwi_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "bert_model = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertForMaskedLM.from_pretrained(bert_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b053ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "zipf_frequency('stop', 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "zipf_frequency('thwart', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_bert_candidates(input_text, list_cwi_predictions, numb_predictions_displayed = 10):\n",
    "  list_candidates_bert = []\n",
    "  for word,pred  in zip(input_text.split(), list_cwi_predictions):\n",
    "    if (pred and (pos_tag([word])[0][1] in ['NNS', 'NN', 'VBP', 'RB', 'VBG','VBD' ]))  or (zipf_frequency(word, 'en')) <3.1:\n",
    "      replace_word_mask = input_text.replace(word, '[MASK]')\n",
    "      text = f'[CLS]{replace_word_mask} [SEP] {input_text} [SEP] '\n",
    "      tokenized_text = tokenizer.tokenize(text)\n",
    "      masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
    "      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "      segments_ids = [0]*len(tokenized_text)\n",
    "      tokens_tensor = torch.tensor([indexed_tokens])\n",
    "      segments_tensors = torch.tensor([segments_ids])\n",
    "      # Predict all tokens\n",
    "      with torch.no_grad():\n",
    "          outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "          predictions = outputs[0][0][masked_index]\n",
    "      predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]\n",
    "      predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
    "      list_candidates_bert.append((word, predicted_tokens))\n",
    "  return list_candidates_bert\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_texts = [ \n",
    " 'The Risk That Students Could Arrive at School With the Coronavirus As schools grapple with how to reopen, new estimates show that large parts of the country would probably see infected students if classrooms opened now.',\n",
    " 'How a photograph of a young man cradling his dying friend sent me on a journey across India.',\n",
    " 'Pro-democracy parties, which had hoped to ride widespread discontent to big gains, saw the yearlong delay as an attempt to thwart them.',\n",
    " 'Night after night, calm gave way to chaos. See what happened between the protesters and the federal agents.',\n",
    " 'Contact Tracing Is Failing in Many States. Here is Why. Inadequate testing and protracted delays in producing results have crippled tracking and hampered efforts to contain major outbreaks.',\n",
    " 'After an initial decrease in the youth detention population, the rate of release has slowed, and the gap between white youth and Black youth has grown.'\n",
    " 'A laboratory experiment hints at some of the ways the virus might elude antibody treatments. Combining therapies could help, experts said.',\n",
    " 'Though I may not be here with you, I urge you to answer the highest calling of your heart and stand up for what you truly believe.',\n",
    " 'The research does not prove that infected children are contagious, but it should influence the debate about reopening schools, some experts said.',\n",
    " 'Dropping antibody counts are not a sign that our immune system is failing against the coronavirus, nor an omen that we can not develop a viable vaccine.',\n",
    " 'The Senate majority leader has said he will not approve a stimulus package without a “liability shield,” but top White House officials say they do not see it as essential.',\n",
    " 'Campaign efforts to refocus come as the president continues to push divisive messages that have frustrated his own party.'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text in list_texts:\n",
    "  new_text = input_text\n",
    "  input_padded, index_list, len_list = process_input(input_text)\n",
    "  pred_cwi = model.predict(input_padded)\n",
    "  pred_cwi_binary = np.argmax(pred_cwi, axis = 2)\n",
    "  complete_cwi_predictions = complete_missing_word(pred_cwi_binary, index_list, len_list)\n",
    "  bert_candidates =   get_bert_candidates(input_text, complete_cwi_predictions)\n",
    "  for word_to_replace, l_candidates in bert_candidates:\n",
    "    tuples_word_zipf = []\n",
    "    for w in l_candidates:\n",
    "      if w.isalpha():\n",
    "        tuples_word_zipf.append((w, zipf_frequency(w, 'en')))\n",
    "    tuples_word_zipf = sorted(tuples_word_zipf, key = lambda x: x[1], reverse=True)\n",
    "    new_text = re.sub(word_to_replace, tuples_word_zipf[0][0], new_text) \n",
    "  print(\"Original text: \", input_text )\n",
    "  print(\"Simplified text:\", new_text, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
